{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d8b309a",
   "metadata": {},
   "source": [
    "# TF-IDF Index: Theory, Implementation, and Demonstration\n",
    "\n",
    "This notebook provides a comprehensive overview of **Term Frequency-Inverse Document Frequency (TF-IDF)** indexing, how it works, and practical demonstrations using real code.\n",
    "\n",
    "## Table of Contents\n",
    "1. [What is TF-IDF?](#what-is-tf-idf)\n",
    "2. [Mathematical Foundation](#mathematical-foundation)\n",
    "3. [Step-by-Step Manual Calculation](#step-by-step-manual-calculation)\n",
    "4. [Implementation with TFIDFCalculator](#implementation-with-tfidfc-alculator)\n",
    "5. [Practical Applications](#practical-applications)\n",
    "6. [Visualization and Analysis](#visualization-and-analysis)\n",
    "7. [Use Cases in Information Retrieval](#use-cases-in-information-retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3561600",
   "metadata": {},
   "source": [
    "## What is TF-IDF?\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** is a numerical statistic used to reflect how important a word is to a document in a collection or corpus of documents. It's widely used in:\n",
    "\n",
    "- **Information Retrieval**: Search engines use TF-IDF to rank documents\n",
    "- **Text Mining**: To identify key terms in documents\n",
    "- **Document Classification**: As features for machine learning models\n",
    "- **RAG Systems**: For keyword-based retrieval in Retrieval-Augmented Generation\n",
    "\n",
    "### Key Intuition:\n",
    "- **High TF-IDF**: A word appears frequently in a specific document but rarely across the entire corpus\n",
    "- **Low TF-IDF**: A word either appears rarely in the document, or appears commonly across many documents (like \"the\", \"and\")\n",
    "\n",
    "TF-IDF helps identify **distinctive** and **relevant** terms for each document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495aa8d5",
   "metadata": {},
   "source": [
    "## Mathematical Foundation\n",
    "\n",
    "TF-IDF is the product of two components:\n",
    "\n",
    "### 1. Term Frequency (TF)\n",
    "Measures how frequently a term appears in a document:\n",
    "\n",
    "$$TF(t,d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}$$\n",
    "\n",
    "### 2. Inverse Document Frequency (IDF)\n",
    "Measures how rare or common a term is across all documents:\n",
    "\n",
    "$$IDF(t,D) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t}\\right)$$\n",
    "\n",
    "### 3. TF-IDF Score\n",
    "The final score combines both components:\n",
    "\n",
    "$$\\text{TF-IDF}(t,d,D) = TF(t,d) \\times IDF(t,D)$$\n",
    "\n",
    "Where:\n",
    "- `t` = term (word)\n",
    "- `d` = document\n",
    "- `D` = corpus (collection of all documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39360048",
   "metadata": {},
   "source": [
    "## Step-by-Step Manual Calculation\n",
    "\n",
    "Let's calculate TF-IDF manually for a small corpus to understand the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2414372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06afeda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a small corpus for demonstration\n",
    "documents = [\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"Natural language processing helps computers understand text\",\n",
    "    \"Deep learning uses neural networks with multiple layers\", \n",
    "    \"Machine learning algorithms can learn from data\",\n",
    "    \"Artificial intelligence includes machine learning and deep learning\"\n",
    "]\n",
    "\n",
    "print(\"Document Corpus:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"Doc {i}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962cc54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocessing - Convert to lowercase and tokenize\n",
    "def preprocess(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "processed_docs = [preprocess(doc) for doc in documents]\n",
    "\n",
    "print(\"Preprocessed Documents (tokenized):\")\n",
    "for i, doc in enumerate(processed_docs, 1):\n",
    "    print(f\"Doc {i}: {doc}\")\n",
    "    \n",
    "# Build vocabulary\n",
    "vocabulary = set()\n",
    "for doc in processed_docs:\n",
    "    vocabulary.update(doc)\n",
    "\n",
    "vocabulary = sorted(list(vocabulary))\n",
    "print(f\"\\nVocabulary ({len(vocabulary)} unique terms):\")\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate Term Frequency (TF) for each document\n",
    "def calculate_tf(term, document):\n",
    "    \"\"\"Calculate TF for a term in a document\"\"\"\n",
    "    return document.count(term) / len(document)\n",
    "\n",
    "# Create TF matrix\n",
    "tf_matrix = []\n",
    "for doc in processed_docs:\n",
    "    tf_scores = []\n",
    "    for term in vocabulary:\n",
    "        tf_scores.append(calculate_tf(term, doc))\n",
    "    tf_matrix.append(tf_scores)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "tf_df = pd.DataFrame(tf_matrix, \n",
    "                     columns=vocabulary, \n",
    "                     index=[f'Doc {i+1}' for i in range(len(documents))])\n",
    "\n",
    "print(\"Term Frequency (TF) Matrix:\")\n",
    "print(tf_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f163e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate Inverse Document Frequency (IDF)\n",
    "def calculate_idf(term, documents):\n",
    "    \"\"\"Calculate IDF for a term across all documents\"\"\"\n",
    "    total_docs = len(documents)\n",
    "    docs_containing_term = sum(1 for doc in documents if term in doc)\n",
    "    \n",
    "    if docs_containing_term == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return math.log(total_docs / docs_containing_term)\n",
    "\n",
    "# Calculate IDF for each term\n",
    "idf_scores = {}\n",
    "for term in vocabulary:\n",
    "    idf_scores[term] = calculate_idf(term, processed_docs)\n",
    "\n",
    "# Create IDF DataFrame\n",
    "idf_df = pd.DataFrame(list(idf_scores.items()), \n",
    "                      columns=['Term', 'IDF'])\n",
    "idf_df = idf_df.set_index('Term')\n",
    "\n",
    "print(\"Inverse Document Frequency (IDF) Scores:\")\n",
    "print(idf_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Calculate TF-IDF scores\n",
    "tfidf_matrix = []\n",
    "for i, doc in enumerate(processed_docs):\n",
    "    tfidf_scores = []\n",
    "    for term in vocabulary:\n",
    "        tf = calculate_tf(term, doc)\n",
    "        idf = idf_scores[term]\n",
    "        tfidf = tf * idf\n",
    "        tfidf_scores.append(tfidf)\n",
    "    tfidf_matrix.append(tfidf_scores)\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix,\n",
    "                        columns=vocabulary,\n",
    "                        index=[f'Doc {i+1}' for i in range(len(documents))])\n",
    "\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2edd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine a specific example: term \"machine\" in Document 1\n",
    "example_term = \"machine\"\n",
    "example_doc_idx = 0\n",
    "example_doc = processed_docs[example_doc_idx]\n",
    "\n",
    "print(f\"Detailed Calculation for '{example_term}' in Document {example_doc_idx + 1}:\")\n",
    "print(f\"Document: {documents[example_doc_idx]}\")\n",
    "print(f\"Tokenized: {example_doc}\")\n",
    "print()\n",
    "\n",
    "# Calculate components\n",
    "term_count = example_doc.count(example_term)\n",
    "total_terms = len(example_doc)\n",
    "tf_score = calculate_tf(example_term, example_doc)\n",
    "\n",
    "docs_with_term = sum(1 for doc in processed_docs if example_term in doc)\n",
    "total_docs = len(processed_docs)\n",
    "idf_score = calculate_idf(example_term, processed_docs)\n",
    "\n",
    "tfidf_score = tf_score * idf_score\n",
    "\n",
    "print(f\"TF Calculation:\")\n",
    "print(f\"  - Term '{example_term}' appears {term_count} times\")\n",
    "print(f\"  - Document has {total_terms} total terms\")\n",
    "print(f\"  - TF = {term_count}/{total_terms} = {tf_score:.6f}\")\n",
    "print()\n",
    "\n",
    "print(f\"IDF Calculation:\")\n",
    "print(f\"  - Total documents: {total_docs}\")\n",
    "print(f\"  - Documents containing '{example_term}': {docs_with_term}\")\n",
    "print(f\"  - IDF = log({total_docs}/{docs_with_term}) = {idf_score:.6f}\")\n",
    "print()\n",
    "\n",
    "print(f\"TF-IDF Score:\")\n",
    "print(f\"  - TF-IDF = {tf_score:.6f} × {idf_score:.6f} = {tfidf_score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0e4fc",
   "metadata": {},
   "source": [
    "## Implementation with TFIDFCalculator\n",
    "\n",
    "Now let's use the existing `TFIDFCalculator` class from our workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the tf-idf directory to Python path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the tf-idf module path\n",
    "tfidf_path = '/home/chuan/Documents/My_Study/DB/Vector-Databases/indexes/tf-idf'\n",
    "if tfidf_path not in sys.path:\n",
    "    sys.path.append(tfidf_path)\n",
    "\n",
    "# Import the TFIDFCalculator\n",
    "from tfidf_calculator import TFIDFCalculator\n",
    "\n",
    "print(\"Successfully imported TFIDFCalculator from workspace!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the TF-IDF calculator\n",
    "tfidf_calc = TFIDFCalculator()\n",
    "tfidf_calc.fit(documents)\n",
    "\n",
    "print(\"TFIDFCalculator fitted on corpus.\")\n",
    "print(f\"Vocabulary size: {len(tfidf_calc.vocabulary)}\")\n",
    "print(f\"Number of documents: {len(tfidf_calc.documents)}\")\n",
    "\n",
    "# Show some vocabulary\n",
    "print(f\"\\nSample vocabulary: {sorted(list(tfidf_calc.vocabulary))[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675551ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a query sentence\n",
    "query = \"I want to learn about machine learning algorithms\"\n",
    "print(f\"Query: '{query}'\")\n",
    "\n",
    "# Calculate TF-IDF vector for the query\n",
    "tfidf_vector = tfidf_calc.calculate_tfidf_vector(query)\n",
    "\n",
    "print(f\"\\nTF-IDF scores for query terms:\")\n",
    "if tfidf_vector:\n",
    "    for term, score in sorted(tfidf_vector.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{term:12} | {score:.6f}\")\n",
    "else:\n",
    "    print(\"No matching terms found in corpus vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab33c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top keywords from the query\n",
    "top_keywords = tfidf_calc.get_top_keywords(query, top_k=5)\n",
    "\n",
    "print(f\"Top {len(top_keywords)} keywords from query (ranked by TF-IDF):\")\n",
    "for i, (keyword, score) in enumerate(top_keywords, 1):\n",
    "    print(f\"{i}. {keyword:12} | {score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20305739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare documents using cosine similarity based on TF-IDF\n",
    "def calculate_document_similarity(tfidf_calc, query, documents):\n",
    "    \"\"\"Calculate similarity between query and documents using TF-IDF vectors\"\"\"\n",
    "    query_vector = tfidf_calc.calculate_tfidf_vector(query)\n",
    "    \n",
    "    similarities = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        doc_vector = tfidf_calc.calculate_tfidf_vector(doc)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        common_terms = set(query_vector.keys()) & set(doc_vector.keys())\n",
    "        \n",
    "        if not common_terms:\n",
    "            similarity = 0.0\n",
    "        else:\n",
    "            dot_product = sum(query_vector[term] * doc_vector[term] for term in common_terms)\n",
    "            query_norm = math.sqrt(sum(score**2 for score in query_vector.values()))\n",
    "            doc_norm = math.sqrt(sum(score**2 for score in doc_vector.values()))\n",
    "            \n",
    "            if query_norm == 0 or doc_norm == 0:\n",
    "                similarity = 0.0\n",
    "            else:\n",
    "                similarity = dot_product / (query_norm * doc_norm)\n",
    "        \n",
    "        similarities.append((i, similarity, doc))\n",
    "    \n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Calculate document similarities\n",
    "similarities = calculate_document_similarity(tfidf_calc, query, documents)\n",
    "\n",
    "print(f\"Document ranking for query: '{query}'\")\n",
    "print(\"=\" * 70)\n",
    "for rank, (doc_idx, similarity, doc) in enumerate(similarities, 1):\n",
    "    print(f\"Rank {rank}: Doc {doc_idx + 1} (Similarity: {similarity:.4f})\")\n",
    "    print(f\"    {doc}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc6bed",
   "metadata": {},
   "source": [
    "## Visualization and Analysis\n",
    "\n",
    "Let's create visualizations to better understand TF-IDF behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11968f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of TF-IDF scores\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Filter out terms with all zero scores for better visualization\n",
    "non_zero_cols = tfidf_df.columns[tfidf_df.sum() > 0]\n",
    "filtered_tfidf = tfidf_df[non_zero_cols]\n",
    "\n",
    "sns.heatmap(filtered_tfidf, annot=True, cmap='YlOrRd', fmt='.3f', \n",
    "            cbar_kws={'label': 'TF-IDF Score'})\n",
    "plt.title('TF-IDF Scores Heatmap\\n(Higher scores indicate more important terms for each document)', \n",
    "          fontsize=14, pad=20)\n",
    "plt.xlabel('Terms', fontsize=12)\n",
    "plt.ylabel('Documents', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9938969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize IDF scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "idf_plot_data = idf_df.sort_values('IDF', ascending=True)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(idf_plot_data)))\n",
    "\n",
    "bars = plt.barh(range(len(idf_plot_data)), idf_plot_data['IDF'], color=colors)\n",
    "plt.yticks(range(len(idf_plot_data)), idf_plot_data.index)\n",
    "plt.xlabel('IDF Score', fontsize=12)\n",
    "plt.title('Inverse Document Frequency (IDF) Scores\\n(Higher scores = rarer terms)', \n",
    "          fontsize=14, pad=20)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.3f}', ha='left', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51517be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze term distribution across documents\n",
    "term_doc_counts = {}\n",
    "for term in vocabulary:\n",
    "    count = sum(1 for doc in processed_docs if term in doc)\n",
    "    term_doc_counts[term] = count\n",
    "\n",
    "# Create distribution plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "doc_counts = list(term_doc_counts.values())\n",
    "plt.hist(doc_counts, bins=range(1, max(doc_counts) + 2), alpha=0.7, \n",
    "         color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Number of Documents Containing Term', fontsize=12)\n",
    "plt.ylabel('Number of Terms', fontsize=12)\n",
    "plt.title('Distribution of Terms Across Documents\\n(Shows how many terms appear in 1, 2, 3... documents)', \n",
    "          fontsize=14, pad=20)\n",
    "plt.xticks(range(1, max(doc_counts) + 1))\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(1, max(doc_counts) + 1):\n",
    "    count = doc_counts.count(i)\n",
    "    if count > 0:\n",
    "        plt.text(i, count + 0.1, str(count), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Term distribution analysis:\")\n",
    "for i in range(1, max(doc_counts) + 1):\n",
    "    count = doc_counts.count(i)\n",
    "    terms = [term for term, doc_count in term_doc_counts.items() if doc_count == i]\n",
    "    print(f\"  {count} terms appear in {i} document(s): {terms[:5]}{'...' if len(terms) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c880607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare TF vs TF-IDF for a specific document\n",
    "doc_idx = 0  # First document\n",
    "doc_text = documents[doc_idx]\n",
    "\n",
    "# Get TF and TF-IDF scores for this document\n",
    "tf_scores = tf_df.iloc[doc_idx]\n",
    "tfidf_scores = tfidf_df.iloc[doc_idx]\n",
    "\n",
    "# Filter non-zero scores\n",
    "non_zero_terms = tf_scores[tf_scores > 0].index\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(non_zero_terms))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, tf_scores[non_zero_terms], width, \n",
    "        label='TF (Term Frequency)', alpha=0.8, color='lightblue')\n",
    "plt.bar(x + width/2, tfidf_scores[non_zero_terms], width, \n",
    "        label='TF-IDF', alpha=0.8, color='orange')\n",
    "\n",
    "plt.xlabel('Terms', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title(f'TF vs TF-IDF Comparison for Document 1\\n\"{doc_text}\"', \n",
    "          fontsize=14, pad=20)\n",
    "plt.xticks(x, non_zero_terms, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how TF-IDF reduces scores for common words and emphasizes distinctive terms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcf98e9",
   "metadata": {},
   "source": [
    "## Use Cases in Information Retrieval\n",
    "\n",
    "Let's demonstrate how TF-IDF is used in practical information retrieval scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c277830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger, more realistic corpus for RAG-style retrieval\n",
    "knowledge_base = [\n",
    "    \"Machine learning is a method of data analysis that automates analytical model building using algorithms that iteratively learn from data\",\n",
    "    \"Deep learning is a subset of machine learning with neural networks that have three or more layers mimicking human brain function\",\n",
    "    \"Natural language processing enables computers to understand interpret and manipulate human language using computational linguistics\",\n",
    "    \"Computer vision allows machines to identify and analyze visual content in images and videos using deep learning techniques\",\n",
    "    \"Reinforcement learning is a type of machine learning where agents learn optimal actions through trial and error interactions with environment\",\n",
    "    \"Supervised learning uses labeled training data to learn mapping function from input variables to output variables for predictions\",\n",
    "    \"Unsupervised learning finds hidden patterns in data without labeled examples using clustering dimensionality reduction techniques\",\n",
    "    \"Neural networks are computing systems inspired by biological neural networks consisting of interconnected nodes called neurons\",\n",
    "    \"Artificial intelligence is the simulation of human intelligence in machines programmed to think learn and problem solve\",\n",
    "    \"Data science combines domain expertise programming skills mathematics and statistics to extract insights from data\"\n",
    "]\n",
    "\n",
    "print(f\"Knowledge Base ({len(knowledge_base)} documents):\")\n",
    "for i, doc in enumerate(knowledge_base, 1):\n",
    "    print(f\"{i:2d}. {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF for the larger corpus\n",
    "rag_tfidf = TFIDFCalculator()\n",
    "rag_tfidf.fit(knowledge_base)\n",
    "\n",
    "print(f\"RAG TF-IDF Calculator initialized:\")\n",
    "print(f\"  - Vocabulary size: {len(rag_tfidf.vocabulary)}\")\n",
    "print(f\"  - Documents: {len(rag_tfidf.documents)}\")\n",
    "\n",
    "# Test with different types of queries\n",
    "test_queries = [\n",
    "    \"What is deep learning and neural networks?\",\n",
    "    \"How does supervised learning work with labeled data?\",\n",
    "    \"Explain computer vision and image analysis\",\n",
    "    \"What are clustering techniques in unsupervised learning?\"\n",
    "]\n",
    "\n",
    "print(f\"\\nTest queries:\")\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"{i}. {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3821d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform document retrieval\n",
    "def retrieve_documents(tfidf_calc, query, documents, top_k=3):\n",
    "    \"\"\"Retrieve top-k most relevant documents for a query using TF-IDF\"\"\"\n",
    "    similarities = calculate_document_similarity(tfidf_calc, query, documents)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Test retrieval for each query\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"QUERY {i}: {query}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get top keywords from query\n",
    "    keywords = rag_tfidf.get_top_keywords(query, top_k=3)\n",
    "    print(f\"Key terms: {', '.join([kw[0] for kw in keywords])}\")\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    results = retrieve_documents(rag_tfidf, query, knowledge_base, top_k=3)\n",
    "    \n",
    "    print(f\"\\nTop 3 relevant documents:\")\n",
    "    for rank, (doc_idx, similarity, doc) in enumerate(results, 1):\n",
    "        print(f\"\\n{rank}. [Doc {doc_idx + 1}] Similarity: {similarity:.4f}\")\n",
    "        print(f\"   {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b260b1",
   "metadata": {},
   "source": [
    "## Summary: TF-IDF Strengths and Limitations\n",
    "\n",
    "### ✅ Strengths:\n",
    "1. **Simple and Interpretable**: Easy to understand and implement\n",
    "2. **Fast Computation**: Efficient for large document collections\n",
    "3. **Keyword-focused**: Good for exact keyword matching\n",
    "4. **Reduces Common Word Impact**: Automatically handles stop words through IDF\n",
    "5. **Established Method**: Well-tested in information retrieval systems\n",
    "\n",
    "### ⚠️ Limitations:\n",
    "1. **No Semantic Understanding**: Cannot capture meaning or context\n",
    "2. **Sparse Representations**: Most values are zero in large vocabularies\n",
    "3. **Order Independence**: Ignores word order and phrase structure\n",
    "4. **Synonym Problem**: Cannot match semantically similar terms\n",
    "5. **Fixed Vocabulary**: Cannot handle out-of-vocabulary terms\n",
    "\n",
    "### 🔄 Modern Alternatives:\n",
    "- **Dense Embeddings**: Word2Vec, GloVe, FastText\n",
    "- **Transformer Models**: BERT, RoBERTa, sentence-transformers\n",
    "- **Hybrid Approaches**: Combining TF-IDF with neural embeddings\n",
    "\n",
    "### 🎯 Best Use Cases:\n",
    "- **Keyword Search**: When exact term matching is important\n",
    "- **Document Classification**: As features for ML models\n",
    "- **Baseline Systems**: Starting point for information retrieval\n",
    "- **Hybrid RAG**: Combined with semantic search for comprehensive retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b73019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: Compare keyword vs semantic queries\n",
    "print(\"TF-IDF Behavior Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test cases showing TF-IDF strengths and weaknesses\n",
    "test_cases = [\n",
    "    (\"machine learning algorithms\", \"Exact keyword match - should work well\"),\n",
    "    (\"ML algorithms\", \"Abbreviation - might not match well\"),\n",
    "    (\"artificial intelligence systems\", \"Related concepts - partial match expected\"),\n",
    "    (\"computer programs that learn\", \"Semantic similarity - may not match well\")\n",
    "]\n",
    "\n",
    "for query, description in test_cases:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(f\"Expected: {description}\")\n",
    "    \n",
    "    # Get TF-IDF scores\n",
    "    tfidf_vector = rag_tfidf.calculate_tfidf_vector(query)\n",
    "    \n",
    "    if tfidf_vector:\n",
    "        top_terms = sorted(tfidf_vector.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        print(f\"Top matching terms: {', '.join([f'{term}({score:.3f})' for term, score in top_terms])}\")\n",
    "    else:\n",
    "        print(\"No matching terms found in vocabulary!\")\n",
    "    \n",
    "    # Get best matching document\n",
    "    results = retrieve_documents(rag_tfidf, query, knowledge_base, top_k=1)\n",
    "    if results:\n",
    "        doc_idx, similarity, doc = results[0]\n",
    "        print(f\"Best match (similarity: {similarity:.3f}): {doc[:60]}...\")\n",
    "    else:\n",
    "        print(\"No matching documents found!\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"This demonstrates both the power and limitations of TF-IDF indexing!\")\n",
    "print(\"For semantic understanding, consider using dense embeddings or transformer models.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
